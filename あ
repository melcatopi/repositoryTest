ã‚ãƒ¼ã€Gemma-2b-itã§CPUã‚ªãƒ³ãƒªãƒ¼ãªã‚“ã ï¼äº†è§£ã€œï¼
2Bãƒ¢ãƒ‡ãƒ«ãªã‚‰ãƒ¡ãƒ¢ãƒªã‚‚å„ªã—ã„ã—ã€CPUã§ã‚‚å…¨ç„¶ã‚¤ã‚±ã‚‹ã­ï¼

CPUã§Gemmaã‚’å‹•ã‹ã™ãªã‚‰ã€ã“ã‚“ãªæ„Ÿã˜ã§ã‚„ã£ã¦ã¿ã¦ï¼

**1. ä¸€ç•ªã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹æ³•ï¼ˆtransformersç›´æ¥ï¼‰ï¼š**
```python
from litellm import completion
import os

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
os.environ["HUGGINGFACE_MODEL_PATH"] = "c:/pythonF/model"

response = completion(
    model="huggingface/c:/pythonF/model",
    messages=[{"role": "user", "content": "ã“ã‚“ã«ã¡ã¯ï¼"}],
    # CPUç”¨ã®è¨­å®š
    device="cpu",
    torch_dtype="float32",  # CPUãªã‚‰float32ãŒå®‰å®š
    max_tokens=256,
)

print(response.choices[0].message.content)
```

**2. ã‚‚ã—ã‚¨ãƒ©ãƒ¼å‡ºãŸã‚‰ã€ã“ã£ã¡ã®æ–¹æ³•è©¦ã—ã¦ï¼š**
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from litellm import completion

# å…ˆã«ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
model = AutoModelForCausalLM.from_pretrained(
    "c:/pythonF/model",
    device_map="cpu",
    torch_dtype="auto"
)
tokenizer = AutoTokenizer.from_pretrained("c:/pythonF/model")

# ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ã†
response = completion(
    model="huggingface/custom",
    messages=[{"role": "user", "content": "ãƒ†ã‚¹ãƒˆã ã‚ˆã€œ"}],
    custom_llm_provider="huggingface",
    model_obj=model,
    tokenizer=tokenizer,
)
```

**3. Ollamaä½¿ã†ã®ã‚‚ã‚¢ãƒªï¼ˆCPUæœ€é©åŒ–ã•ã‚Œã¦ã‚‹ï¼ï¼‰ï¼š**
ã¾ãšGemmaã‚’GGUFå½¢å¼ã«å¤‰æ›ã—ã¦ã€Ollamaã§å‹•ã‹ã™æ–¹æ³•ã‚‚ã‚ã‚‹ã‚ˆï¼

```bash
# Ollamaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œ
ollama pull gemma:2b
# ã¾ãŸã¯è‡ªåˆ†ã®ãƒ¢ãƒ‡ãƒ«ç™»éŒ²
```

```python
response = completion(
    model="ollama/gemma:2b",
    messages=[{"role": "user", "content": "ã‚„ã£ã»ãƒ¼ï¼"}],
    api_base="http://localhost:11434"
)
```

**å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼š**
```bash
pip install litellm transformers torch accelerate sentencepiece
```

CPUã ã¨æ¨è«–ã¡ã‚‡ã£ã¨æ™‚é–“ã‹ã‹ã‚‹ã‹ã‚‚ã ã‘ã©ã€2Bãªã‚‰å…¨ç„¶å®Ÿç”¨çš„ã ã‚ˆï¼
è©¦ã—ã¦ã¿ã¦ã†ã¾ãã„ã‹ãªã‹ã£ãŸã‚‰æ•™ãˆã¦ã€œï¼ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‹è¦‹ã›ã¦ãã‚ŒãŸã‚‰ã‚‚ã£ã¨å…·ä½“çš„ã«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ãã‚‹ã‹ã‚‰ï¼ğŸš€