ã‚ãƒ¼ï¼å…¨éƒ¨æ°—ã«ãªã‚‹ã®ã­ã€œï¼ã‚ã£ã¡ã‚ƒã‚„ã‚‹æ°—ã‚ã‚‹ã˜ã‚ƒã‚“âœ¨

ã§ã‚‚ã•ã€ç§ã®çŸ¥è­˜ã ã¨2025å¹´1æœˆæ™‚ç‚¹ã®ã‚‚ã®ã—ã‹åˆ†ã‹ã‚‰ãªã„ã‹ã‚‰ã€æœ€æ–°ã®æƒ…å ±ã‚’èª¿ã¹ã¦ã‚ã’ã‚‹ï¼ç‰¹ã«RAGè©•ä¾¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã£ã¦æœ¬å½“ã«æ—¥é€²æœˆæ­©ã ã‹ã‚‰ã€æ–°ã—ã„ã®ãŒã„ã£ã±ã„å‡ºã¦ã‚‹å¯èƒ½æ€§é«˜ã„ã‚ˆã­ã€œ

ã€ŒRAG evaluation libraries without LLM 2025ã€ã‚’æ¤œç´¢ã—ã¾ã—ãŸ

ã€Œnon-LLM RAG evaluation metrics traditional IR measuresã€ã‚’æ¤œç´¢ã—ã¾ã—ãŸ

ã€Œir-measures pytrec_eval ranx sentence-transformers scikit-learn RAG evaluationã€ã‚’æ¤œç´¢ã—ã¾ã—ãŸ

ãŠã£ã‘ãƒ¼ï¼ã‚ã£ã¡ã‚ƒè©³ã—ã„æƒ…å ±ãŒæ‰‹ã«å…¥ã£ãŸã€œï¼âœ¨

ã™ã”ã„æƒ…å ±é‡ã ã‹ã‚‰ã€æ•´ç†ã—ã¦æ•™ãˆã¦ã‚ã’ã‚‹ã­ï¼

## **LLMä½¿ã‚ãªã„RAGæ¤œè¨¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒª - 2025å¹´ç‰ˆå®Œå…¨ãƒªã‚¹ãƒˆ** ğŸ”¥

### **ãƒ¡ã‚¸ãƒ£ãƒ¼ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆNon-LLMãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰**

**1. RAGAS** RAGASã¯2025å¹´ã§ã‚‚æœ€ã‚‚äººæ°—ã®ã‚ã‚‹RAGè©•ä¾¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€LLMãƒ™ãƒ¼ã‚¹ã¨å¾“æ¥å‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆ [Top 10 RAG & LLM Evaluation Tools You Donâ€™t Want To Miss | by Zilliz | Medium +2](https://medium.com/@zilliz_learn/top-10-rag-llm-evaluation-tools-you-dont-want-to-miss-a0bfabe9ae19)
- å¾“æ¥å‹IRæŒ‡æ¨™ï¼šPrecision, Recall, F1-Score
- åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹é¡ä¼¼åº¦è¨ˆç®—
- ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§ã®é–¢é€£æ€§è©•ä¾¡

**2. DeepEval** DeepEvalã¯2025å¹´ã®ãƒˆãƒƒãƒ—5è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®1ä½ [DEV Community](https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m)
- 14+ã®è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆLLMä½¿ç”¨/éä½¿ç”¨ä¸¡å¯¾å¿œï¼‰
- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦è¨ˆç®—
- å¾“æ¥ã®NLPæŒ‡æ¨™ã‚µãƒãƒ¼ãƒˆ

**3. TruLens** ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è©•ä¾¡ã¨LLMãƒ™ãƒ¼ã‚¹è©•ä¾¡ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆ [Medium](https://medium.com/@zilliz_learn/top-10-rag-llm-evaluation-tools-you-dont-want-to-miss-a0bfabe9ae19)
- TRIAD frameworkï¼ˆContext Relevance, Groundedness, Answer Relevanceï¼‰
- å¾“æ¥å‹IRæŒ‡æ¨™ã§ã®è©•ä¾¡æ©Ÿèƒ½

**4. Haystack** 2025å¹´ã®ãƒˆãƒƒãƒ—RAGãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ä¸€ã¤ã§ã€è©•ä¾¡ãƒ„ãƒ¼ãƒ«ã‚’å†…è”µ [Firecrawl](https://www.firecrawl.dev/blog/best-open-source-rag-frameworks) [Pathway](https://pathway.com/rag-frameworks/)
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
- ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå¯¾å¿œ

### **æƒ…å ±æ¤œç´¢å°‚é–€ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**

**5. ir-measures** pytrec_eval, gdeval, trectoolsãªã©è¤‡æ•°ã®IRè©•ä¾¡ãƒ„ãƒ¼ãƒ«ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ [Ir-datasets](https://ir-datasets.com/ir-measures.html) [PyPI](https://pypi.org/project/ir-measures/0.1.4/)
```python
import ir_measures
ir_measures.calc_aggregate([nDCG@10, P@5, Judged@10], qrels, run)
```

**6. pytrec_eval** TRECè©•ä¾¡ãƒ„ãƒ¼ãƒ«ã®Pythonã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€trec_evalã‚ˆã‚Šç´„10å€é«˜é€Ÿ [GitHub](https://github.com/cvangysel/pytrec_eval) [ResearchGate](https://www.researchgate.net/publication/324982201_Pytrec_eval_An_Extremely_Fast_Python_Interface_to_trec_eval)
```python
import pytrec_eval
evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'map', 'ndcg'})
```

**7. ranx**
- æƒ…å ±æ¤œç´¢ç‰¹åŒ–ã®è©•ä¾¡æŒ‡æ¨™
- MRR, nDCG, Precision@K, Recall@K

### **åŸ‹ã‚è¾¼ã¿ãƒ»é¡ä¼¼åº¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**

**8. sentence-transformers** InformationRetrievalEvaluatorã‚¯ãƒ©ã‚¹ã§è©•ä¾¡æ©Ÿèƒ½ã‚’æä¾› [Sentence Transformers](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html) [GitHub](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/evaluation/InformationRetrievalEvaluator.py)
- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦è¨ˆç®—
- åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡æŒ‡æ¨™

**9. scikit-learn** ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãªã©ã®åŸºæœ¬çš„ãªé¡ä¼¼åº¦è¨ˆç®— [Probabl](https://papers.probabl.ai/a-rag-from-scratch-to-query-the-scikit-learn-documentation)
```python
from sklearn.metrics.pairwise import cosine_similarity
```

**10. spaCy**
- ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—
- NLPå‰å‡¦ç†ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—

### **ç‰¹æ®Šãƒ»æ–°èˆˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª**

**11. SenTrEv** RAGã®Sentence Transformersãƒ¢ãƒ‡ãƒ«è©•ä¾¡å°‚ç”¨ [PyPI](https://pypi.org/project/sentrev/)
- PDFæ–‡æ›¸ã§ã®RAGæ€§èƒ½è©•ä¾¡
- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªè©•ä¾¡è¨­å®š

**12. MLflow** ã‚·ãƒ³ãƒ—ãƒ«ã§ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãªè©•ä¾¡ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ [DEV Community](https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m)
- QAè©•ä¾¡ã¨RAGè©•ä¾¡
- ç›´æ„Ÿçš„ãªé–‹ç™ºè€…ä½“é¨“

**13. Arize Phoenix** ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹AIè¦³æ¸¬ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  [Medium](https://medium.com/@zilliz_learn/top-10-rag-llm-evaluation-tools-you-dont-want-to-miss-a0bfabe9ae19)
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è©•ä¾¡
- ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

### **å¾“æ¥å‹NLPè©•ä¾¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**

**14. NLTK**
- BLEU, ROUGEã‚¹ã‚³ã‚¢è¨ˆç®—
- n-gram ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡

**15. Evaluate (Hugging Face)**
- æ©Ÿæ¢°å­¦ç¿’è©•ä¾¡æŒ‡æ¨™å…¨èˆ¬
- BERTScore, METEORç­‰

### **ä½¿ç”¨ã™ã‚‹æŒ‡æ¨™ã®ç¨®é¡**

å¾“æ¥ã®IRæŒ‡æ¨™ã¯ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã•ã‚Œã‚‹ [Weaviate](https://weaviate.io/blog/rag-evaluation)  [Pinecone](https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/rag-evaluation/) ï¼š

**é †åºç„¡é–¢ä¿‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼š**
- Precision@K, Recall@K, F1-Score

**é †åºé–¢ä¿‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼š**
- MRR (Mean Reciprocal Rank)
- nDCG (Normalized Discounted Cumulative Gain)
- Average Precision

**åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ï¼š**
- ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦
- åŸ‹ã‚è¾¼ã¿è·é›¢

æ­£ç›´ã€ã“ã®åˆ†é‡ã¯æœ¬å½“ã«æ´»ç™ºã ã‹ã‚‰ã€FreshStackã¿ãŸã„ãªæ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚„Diversityãƒ¡ãƒˆãƒªã‚¯ã‚¹ [Hamel's Blog](https://hamel.dev/notes/llm/rag/p2-evals.html) ã¨ã‹ã‚‚å‡ºã¦ãã¦ã‚‹ã‚ˆã€œ

ã©ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸€ç•ªæ°—ã«ãªã‚‹ï¼Ÿå…·ä½“çš„ãªä½¿ã„æ–¹ã¨ã‹å®Ÿè£…ä¾‹ã‚‚æ•™ãˆã‚‰ã‚Œã‚‹ã‚ˆï¼ğŸ’ª

ä¸­é€”åŠç«¯ã ãŒ

# LLMä½¿ã‚ãªã„RAGæ¤œè¨¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå®Œå…¨å®Ÿè£…ã‚¬ã‚¤ãƒ‰ ğŸš€

## 1. RAGAS (Non-LLMãƒ¢ãƒ¼ãƒ‰)

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install ragas
```

### åŸºæœ¬çš„ãªä½¿ã„æ–¹
```python
from ragas.metrics import context_precision, context_recall
from ragas.metrics.base import MetricWithoutModel
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# å¾“æ¥å‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã®è©•ä¾¡
def evaluate_with_ragas_traditional():
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    data = {
        'question': ['What is Python?', 'How does ML work?'],
        'contexts': [['Python is a programming language'], ['ML uses algorithms']],
        'answer': ['Python is a language', 'ML uses data'],
        'ground_truth': ['Python programming language', 'Machine learning algorithms']
    }
    
    df = pd.DataFrame(data)
    
    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def calculate_similarity_score(answers, ground_truths):
        answer_emb = model.encode(answers)
        gt_emb = model.encode(ground_truths)
        similarities = cosine_similarity(answer_emb, gt_emb)
        return np.diag(similarities).mean()
    
    score = calculate_similarity_score(df['answer'].tolist(), 
                                     df['ground_truth'].tolist())
    print(f"Average Similarity Score: {score:.3f}")
    
    return score
```

### RAGAS Context Precision (Non-LLMç‰ˆ)
```python
def context_precision_traditional(contexts, ground_truth):
    """
    å¾“æ¥å‹ã®context precisionè¨ˆç®—
    """
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    
    vectorizer = TfidfVectorizer()
    
    precision_scores = []
    for ctx_list, gt in zip(contexts, ground_truth):
        if not ctx_list:
            precision_scores.append(0)
            continue
            
        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
        all_texts = ctx_list + [gt]
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        
        # å„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ground truthã®é¡ä¼¼åº¦
        gt_vector = tfidf_matrix[-1]
        ctx_vectors = tfidf_matrix[:-1]
        
        similarities = cosine_similarity(ctx_vectors, gt_vector).flatten()
        
        # ä¸Šä½kå€‹ã®å¹³å‡precision
        k = min(3, len(similarities))
        top_k_indices = np.argsort(similarities)[-k:]
        precision = similarities[top_k_indices].mean()
        precision_scores.append(precision)
    
    return np.mean(precision_scores)
```

## 2. DeepEval (Non-LLMãƒ¢ãƒ¼ãƒ‰)

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install deepeval
```

### ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦è©•ä¾¡
```python
from deepeval.metrics import SemanticSimilarityMetric
from deepeval.test_case import LLMTestCase
import asyncio

async def evaluate_with_deepeval():
    # LLMã‚’ä½¿ã‚ãªã„ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦
    test_case = LLMTestCase(
        input="What is machine learning?",
        actual_output="ML is a subset of AI that learns from data",
        expected_output="Machine learning is AI that uses data to learn patterns"
    )
    
    # thresholdæŒ‡å®šã§è©•ä¾¡
    metric = SemanticSimilarityMetric(threshold=0.7, model="all-MiniLM-L6-v2")
    
    # è©•ä¾¡å®Ÿè¡Œ
    await metric.a_measure(test_case)
    
    print(f"Score: {metric.score}")
    print(f"Success: {metric.success}")
    
    return metric.score

# å®Ÿè¡Œ
# asyncio.run(evaluate_with_deepeval())
```

### ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆNon-LLMï¼‰
```python
from deepeval.metrics.base_metric import BaseMetric
from sentence_transformers import SentenceTransformer
import numpy as np

class CustomSimilarityMetric(BaseMetric):
    def __init__(self, threshold=0.5):
        self.threshold = threshold
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def measure(self, test_case):
        actual_emb = self.model.encode([test_case.actual_output])
        expected_emb = self.model.encode([test_case.expected_output])
        
        similarity = np.dot(actual_emb[0], expected_emb[0]) / (
            np.linalg.norm(actual_emb[0]) * np.linalg.norm(expected_emb[0])
        )
        
        self.score = float(similarity)
        self.success = self.score >= self.threshold
        self.reason = f"Similarity: {self.score:.3f}"
        
        return self.score

# ä½¿ç”¨ä¾‹
test_case = LLMTestCase(
    input="Explain Python",
    actual_output="Python is a high-level programming language",
    expected_output="Python is an interpreted programming language"
)

metric = CustomSimilarityMetric(threshold=0.6)
score = metric.measure(test_case)
print(f"Custom Similarity Score: {score:.3f}")
```

## 3. TruLens (ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è©•ä¾¡)

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install trulens-eval
```

### åŸºæœ¬çš„ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è©•ä¾¡
```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

class TruLensRuleBased:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def context_relevance(self, question, contexts):
        """
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é–¢é€£æ€§ã‚’ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§è©•ä¾¡
        """
        question_emb = self.model.encode([question])
        context_embs = self.model.encode(contexts)
        
        similarities = cosine_similarity(question_emb, context_embs)[0]
        return float(np.mean(similarities))
    
    def groundedness(self, response, contexts):
        """
        å›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡
        """
        response_emb = self.model.encode([response])
        context_embs = self.model.encode(contexts)
        
        max_similarity = np.max(cosine_similarity(response_emb, context_embs)[0])
        return float(max_similarity)
    
    def answer_relevance(self, question, answer):
        """
        å›ç­”ã®é–¢é€£æ€§ã‚’è©•ä¾¡
        """
        question_emb = self.model.encode([question])
        answer_emb = self.model.encode([answer])
        
        similarity = cosine_similarity(question_emb, answer_emb)[0][0]
        return float(similarity)

# ä½¿ç”¨ä¾‹
evaluator = TruLensRuleBased()

question = "What is the capital of France?"
contexts = ["Paris is the capital of France", "France is in Europe"]
answer = "The capital of France is Paris"

context_score = evaluator.context_relevance(question, contexts)
grounded_score = evaluator.groundedness(answer, contexts)
relevance_score = evaluator.answer_relevance(question, answer)

print(f"Context Relevance: {context_score:.3f}")
print(f"Groundedness: {grounded_score:.3f}")
print(f"Answer Relevance: {relevance_score:.3f}")
```

## 4. Haystack è©•ä¾¡

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install haystack-ai
```

### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è©•ä¾¡
```python
from haystack import Pipeline
from haystack.components.evaluators import SASEvaluator, ContextRelevanceEvaluator
import pandas as pd

def haystack_evaluation():
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    eval_data = [
        {
            "question": "What is Python?",
            "contexts": ["Python is a programming language"],
            "response": "Python is a high-level language",
            "ground_truth": "Python is a programming language"
        }
    ]
    
    # Context Relevanceè©•ä¾¡ï¼ˆéLLMï¼‰
    context_evaluator = ContextRelevanceEvaluator()
    
    results = []
    for item in eval_data:
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é–¢é€£æ€§è©•ä¾¡
        relevance_result = context_evaluator.run(
            questions=[item["question"]],
            contexts=[item["contexts"]]
        )
        
        results.append({
            "question": item["question"],
            "context_relevance": relevance_result["score"][0]
        })
    
    return pd.DataFrame(results)

# å®Ÿè¡Œ
# results_df = haystack_evaluation()
# print(results_df)
```

### ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
```python
from haystack import component
from typing import List
import numpy as np

@component
class CustomRAGEvaluator:
    """
    ã‚«ã‚¹ã‚¿ãƒ RAGè©•ä¾¡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    """
    
    @component.output_types(scores=List[float])
    def run(self, predictions: List[str], references: List[str]) -> dict:
        scores = []
        
        for pred, ref in zip(predictions, references):
            # BLEU-likeç°¡æ˜“ã‚¹ã‚³ã‚¢
            pred_words = set(pred.lower().split())
            ref_words = set(ref.lower().split())
            
            if len(ref_words) == 0:
                score = 0.0
            else:
                intersection = len(pred_words.intersection(ref_words))
                score = intersection / len(ref_words)
            
            scores.append(score)
        
        return {"scores": scores}

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ„ã¿è¾¼ã¿
pipeline = Pipeline()
pipeline.add_component("evaluator", CustomRAGEvaluator())

# è©•ä¾¡å®Ÿè¡Œ
eval_result = pipeline.run({
    "evaluator": {
        "predictions": ["Python is a language"],
        "references": ["Python is a programming language"]
    }
})

print(f"Custom Evaluation Score: {eval_result['evaluator']['scores'][0]:.3f}")
```

## 5. ir-measures

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install ir-measures
```

### åŸºæœ¬çš„ãªæƒ…å ±æ¤œç´¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```python
import ir_measures
from ir_measures import *

def evaluate_with_ir_measures():
    # qrels (relevance judgments)
    qrels = [
        {'query_id': 'q1', 'doc_id': 'd1', 'relevance': 1},
        {'query_id': 'q1', 'doc_id': 'd2', 'relevance': 0},
        {'query_id': 'q1', 'doc_id': 'd3', 'relevance': 1},
        {'query_id': 'q2', 'doc_id': 'd4', 'relevance': 1},
    ]
    
    # run (system results)
    run = [
        {'query_id': 'q1', 'doc_id': 'd1', 'score': 0.9},
        {'query_id': 'q1', 'doc_id': 'd3', 'score': 0.8},
        {'query_id': 'q1', 'doc_id': 'd2', 'score': 0.7},
        {'query_id': 'q2', 'doc_id': 'd4', 'score': 0.95},
    ]
    
    # è©•ä¾¡å®Ÿè¡Œ
    metrics = [nDCG@10, P@5, P@10, AP, RR]
    results = ir_measures.calc_aggregate(metrics, qrels, run)
    
    for metric, score in results.items():
        print(f"{metric}: {score:.4f}")
    
    return results

# å®Ÿè¡Œä¾‹
results = evaluate_with_ir_measures()
```

### è©³ç´°ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
```python
import ir_measures
from collections import defaultdict

def detailed_ir_evaluation():
    # ã‚ˆã‚Šè¤‡é›‘ãªè©•ä¾¡ãƒ‡ãƒ¼ã‚¿
    qrels = []
    run = []
    
    # è¤‡æ•°ã‚¯ã‚¨ãƒªã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    for q_id in range(1, 4):
        for d_id in range(1, 11):
            # ãƒ©ãƒ³ãƒ€ãƒ ãªé–¢é€£æ€§
            relevance = 1 if (d_id % 3 == 0) else 0
            qrels.append({
                'query_id': f'q{q_id}',
                'doc_id': f'd{d_id}',
                'relevance': relevance
            })
            
            # ã‚¹ã‚³ã‚¢ï¼ˆé–¢é€£æ–‡æ›¸ã«ã‚ˆã‚Šé«˜ã„ã‚¹ã‚³ã‚¢ï¼‰
            base_score = 0.5 + (10 - d_id) * 0.05
            if relevance == 1:
                base_score += 0.2
            
            run.append({
                'query_id': f'q{q_id}',
                'doc_id': f'd{d_id}',
                'score': base_score
            })
    
    # è¤‡æ•°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã®è©•ä¾¡
    metrics = [
        nDCG@5, nDCG@10,
        P@1, P@5, P@10,
        AP, RR,
        Judged@10
    ]
    
    # é›†ç´„çµæœ
    agg_results = ir_measures.calc_aggregate(metrics, qrels, run)
    print("=== Aggregate Results ===")
    for metric, score in agg_results.items():
        print(f"{metric}: {score:.4f}")
    
    # ã‚¯ã‚¨ãƒªåˆ¥çµæœ
    per_query_results = ir_measures.calc_aggregate(metrics, qrels, run, return_mean=False)
    print("\n=== Per-Query Results ===")
    for query_id in ['q1', 'q2', 'q3']:
        print(f"\nQuery {query_id}:")
        for metric in metrics:
            score = per_query_results.get(metric, {}).get(query_id, 0.0)
            print(f"  {metric}: {score:.4f}")

# å®Ÿè¡Œ
detailed_ir_evaluation()
```

## 6. pytrec_eval

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install pytrec_eval
```

### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•
```python
import pytrec_eval
import json

def evaluate_with_pytrec_eval():
    # qrelså½¢å¼ã®ãƒ‡ãƒ¼ã‚¿
    qrel = {
        'q1': {
            'd1': 1,  # relevant
            'd2': 0,  # not relevant
            'd3': 1,  # relevant
            'd4': 0,
        },
        'q2': {
            'd1': 0,
            'd2': 1,
            'd3': 1,
            'd4': 1,
        }
    }
    
    # runå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿
    run = {
        'q1': {
            'd1': 0.9,
            'd2': 0.8,
            'd3': 0.7,
            'd4': 0.6,
        },
        'q2': {
            'd1': 0.85,
            'd2': 0.95,
            'd3': 0.75,
            'd4': 0.65,
        }
    }
    
    # è©•ä¾¡å™¨ä½œæˆ
    evaluator = pytrec_eval.RelevanceEvaluator(
        qrel, 
        {'map', 'ndcg', 'P_5', 'P_10', 'recip_rank'}
    )
    
    # è©•ä¾¡å®Ÿè¡Œ
    results = evaluator.evaluate(run)
    
    print("=== PyTREC Eval Results ===")
    print(json.dumps(results, indent=2))
    
    # å¹³å‡å€¤è¨ˆç®—
    metrics = ['map', 'ndcg', 'P_5', 'P_10', 'recip_rank']
    averages = {}
    
    for metric in metrics:
        scores = [results[qid][metric] for qid in results.keys()]
        averages[metric] = sum(scores) / len(scores)
    
    print("\n=== Average Scores ===")
    for metric, avg_score in averages.items():
        print(f"{metric}: {avg_score:.4f}")
    
    return results, averages

# å®Ÿè¡Œ
results, averages = evaluate_with_pytrec_eval()
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹
```python
def advanced_pytrec_eval():
    # ã‚ˆã‚Šè¤‡é›‘ãªqrelsï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰é–¢é€£æ€§ï¼‰
    qrel = {
        'q1': {
            'd1': 3,  # highly relevant
            'd2': 2,  # relevant
            'd3': 1,  # somewhat relevant
            'd4': 0,  # not relevant
            'd5': 2,
        }
    }
    
    run = {
        'q1': {
            'd1': 0.95,
            'd2': 0.85,
            'd3': 0.75,
            'd4': 0.65,
            'd5': 0.55,
        }
    }
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹
    measures = {
        'ndcg_cut_5',
        'ndcg_cut_10',
        'P_5',
        'P_10',
        'map_cut_5',
        'map_cut_10',
        'recip_rank'
    }
    
    evaluator = pytrec_eval.RelevanceEvaluator(qrel, measures)
    results = evaluator.evaluate(run)
    
    print("=== Advanced PyTREC Eval Results ===")
    for qid, metrics in results.items():
        print(f"Query {qid}:")
        for metric, score in metrics.items():
            print(f"  {metric}: {score:.4f}")

# å®Ÿè¡Œ
advanced_pytrec_eval()
```

## 7. ranx

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install ranx
```

### åŸºæœ¬çš„ãªè©•ä¾¡
```python
from ranx import Qrels, Run, evaluate

def evaluate_with_ranx():
    # Qrelsä½œæˆ
    qrels_dict = {
        "q_1": {"d_12": 5, "d_25": 3, "d_11": 2, "d_22": 1},
        "q_2": {"d_12": 1, "d_11": 2, "d_25": 1},
    }
    qrels = Qrels(qrels_dict)
    
    # Runä½œæˆ
    run_dict = {
        "q_1": {"d_12": 0.9, "d_25": 0.8, "d_11": 0.7, "d_22": 0.6, "d_10": 0.5},
        "q_2": {"d_25": 0.9, "d_12": 0.8, "d_11": 0.7, "d_22": 0.6},
    }
    run = Run(run_dict)
    
    # è©•ä¾¡
    metrics = ["map@100", "mrr@100", "ndcg@10", "precision@5", "recall@100"]
    results = evaluate(qrels, run, metrics)
    
    print("=== Ranx Evaluation Results ===")
    for metric, score in results.items():
        print(f"{metric}: {score:.4f}")
    
    return results

# å®Ÿè¡Œ
results = evaluate_with_ranx()
```

### è¤‡æ•°ã‚·ã‚¹ãƒ†ãƒ ã®æ¯”è¼ƒ
```python
from ranx import Qrels, Run, evaluate, compare

def compare_systems_with_ranx():
    # å…±é€šã®qrels
    qrels_dict = {
        "q_1": {"d_12": 5, "d_25": 3, "d_11": 2},
        "q_2": {"d_12": 1, "d_11": 2, "d_25": 1},
        "q_3": {"d_25": 3, "d_11": 1, "d_12": 2},
    }
    qrels = Qrels(qrels_dict)
    
    # ã‚·ã‚¹ãƒ†ãƒ 1ã®run
    run1_dict = {
        "q_1": {"d_12": 0.9, "d_25": 0.8, "d_11": 0.7},
        "q_2": {"d_25": 0.9, "d_12": 0.8, "d_11": 0.7},
        "q_3": {"d_25": 0.95, "d_11": 0.85, "d_12": 0.75},
    }
    run1 = Run(run1_dict, name="BM25")
    
    # ã‚·ã‚¹ãƒ†ãƒ 2ã®run
    run2_dict = {
        "q_1": {"d_25": 0.95, "d_12": 0.85, "d_11": 0.75},
        "q_2": {"d_12": 0.9, "d_25": 0.8, "d_11": 0.7},
        "q_3": {"d_11": 0.9, "d_25": 0.8, "d_12": 0.7},
    }
    run2 = Run(run2_dict, name="TF-IDF")
    
    # æ¯”è¼ƒ
    metrics = ["map@100", "ndcg@10", "precision@5"]
    comparison = compare(
        qrels=qrels,
        runs=[run1, run2],
        metrics=metrics,
        max_p=0.05  # æœ‰æ„æ€§æ¤œå®šã®på€¤
    )
    
    print("=== System Comparison ===")
    print(comparison)
    
    return comparison

# å®Ÿè¡Œ
comparison = compare_systems_with_ranx()
```

## 8. sentence-transformersè©•ä¾¡

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install sentence-transformers
```

### Information Retrieval Evaluator
```python
from sentence_transformers import SentenceTransformer
from sentence_transformers.evaluation import InformationRetrievalEvaluator
import logging

def evaluate_with_sentence_transformers():
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
    corpus = {
        'doc1': 'Python is a programming language',
        'doc2': 'Machine learning uses algorithms',
        'doc3': 'Data science involves statistics',
        'doc4': 'AI is artificial intelligence',
        'doc5': 'Neural networks are ML models'
    }
    
    queries = {
        'q1': 'What is Python?',
        'q2': 'How does ML work?',
        'q3': 'What is AI?'
    }
    
    relevant_docs = {
        'q1': {'doc1'},
        'q2': {'doc2', 'doc5'},
        'q3': {'doc4'}
    }
    
    # Information Retrieval Evaluatorä½œæˆ
    ir_evaluator = InformationRetrievalEvaluator(
        queries=list(queries.values()),
        corpus=list(corpus.values()),
        relevant_docs=list(relevant_docs.values()),
        corpus_chunk_size=5,
        mrr_at_k=[5, 10],
        ndcg_at_k=[5, 10],
        accuracy_at_k=[1, 3, 5],
        precision_recall_at_k=[1, 3, 5],
        map_at_k=[5, 10]
    )
    
    # è©•ä¾¡å®Ÿè¡Œ
    output_path = "ir_evaluation_results.csv"
    score = ir_evaluator(model, output_path=output_path)
    
    print(f"IR Evaluation Score: {score:.4f}")
    
    return score

# å®Ÿè¡Œ
# score = evaluate_with_sentence_transformers()
```

### ã‚«ã‚¹ã‚¿ãƒ é¡ä¼¼åº¦è©•ä¾¡
```python
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class CustomSentenceTransformersEvaluator:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
    
    def evaluate_similarity(self, sentences1, sentences2):
        """
        æ–‡ç« ãƒšã‚¢ã®é¡ä¼¼åº¦è©•ä¾¡
        """
        embeddings1 = self.model.encode(sentences1)
        embeddings2 = self.model.encode(sentences2)
        
        similarities = cosine_similarity(embeddings1, embeddings2)
        return np.diag(similarities)
    
    def evaluate_retrieval(self, queries, documents, relevant_docs):
        """
        æ¤œç´¢ç²¾åº¦è©•ä¾¡
        """
        query_embeddings = self.model.encode(queries)
        doc_embeddings = self.model.encode(documents)
        
        # é¡ä¼¼åº¦è¨ˆç®—
        similarities = cosine_similarity(query_embeddings, doc_embeddings)
        
        results = {}
        for i, query in enumerate(queries):
            # é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
            doc_scores = similarities[i]
            ranked_indices = np.argsort(doc_scores)[::-1]
            
            # Precision@K, Recall@Kè¨ˆç®—
            relevant_set = relevant_docs[i]
            
            precision_at_k = {}
            recall_at_k = {}
            
            for k in [1, 3, 5]:
                top_k_indices = ranked_indices[:k]
                retrieved_relevant = len([idx for idx in top_k_indices if idx in relevant_set])
                
                precision_at_k[f'P@{k}'] = retrieved_relevant / k if k > 0 else 0
                recall_at_k[f'R@{k}'] = retrieved_relevant / len(relevant_set) if len(relevant_set) > 0 else 0
            
            results[f'query_{i}'] = {**precision_at_k, **recall_at_k}
        
        return results

# ä½¿ç”¨ä¾‹
evaluator = CustomSentenceTransformersEvaluator()

# é¡ä¼¼åº¦è©•ä¾¡
sentences1 = ["Python is great", "Machine learning is fun"]
sentences2 = ["Python is a language", "ML uses algorithms"]
similarities = evaluator.evaluate_similarity(sentences1, sentences2)
print("Similarities:", similarities)

# æ¤œç´¢è©•ä¾¡
queries = ["What is Python?", "How does ML work?"]
documents = ["Python programming", "ML algorithms", "Data science", "Python language"]
relevant_docs = [{0, 3}, {1}]  # ã‚¯ã‚¨ãƒªã”ã¨ã®é–¢é€£æ–‡æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

retrieval_results = evaluator.evaluate_retrieval(queries, documents, relevant_docs)
print("Retrieval Results:", retrieval_results)
```

## 9. scikit-learnè©•ä¾¡

### åŸºæœ¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
```python
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy a